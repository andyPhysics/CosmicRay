{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import uproot\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, StackingRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from random import random\n",
    "from sklearn.model_selection import cross_validate\n",
    "import matplotlib\n",
    "from waveform_methods import binning\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = uproot.open('12360.root')\n",
    "file2 = uproot.open('12362.root')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172382\n"
     ]
    }
   ],
   "source": [
    "Xmax1 = file['MCPrimaryInfo']['ghMaxDepth'].array()\n",
    "valueDepth = file['MCPrimaryInfo']['longNumCharged'].array()\n",
    "Depth1 = file['MCPrimaryInfo']['longDepth'].array()\n",
    "chi2_1 = file['CurvatureOnlyParams']['chi2_time'].array()\n",
    "red1 = file['MCPrimaryInfo']['ghRedChiSqr'].array()\n",
    "mass1 = [1 for i in range(len(red1))]\n",
    "\n",
    "good = []\n",
    "count = 0\n",
    "for i in range(len(Xmax1)):\n",
    "    if (red1[i]<300 and chi2_1[i]<5) and Xmax1[i]<900:\n",
    "        good.append(1)\n",
    "        count+=1\n",
    "    else:\n",
    "        good.append(0)\n",
    "        \n",
    "Xmax2 = file2['MCPrimaryInfo']['ghMaxDepth'].array()\n",
    "valueDepth = file2['MCPrimaryInfo']['longNumCharged'].array()\n",
    "Depth2 = file2['MCPrimaryInfo']['longDepth'].array()\n",
    "chi2_2 = file2['CurvatureOnlyParams']['chi2_time'].array()\n",
    "red2 = file2['MCPrimaryInfo']['ghRedChiSqr'].array()\n",
    "mass2 = [4 for i in range(len(red2))]\n",
    "\n",
    "good2 = []\n",
    "for i in range(len(Xmax2)):\n",
    "    if (red2[i]<300 and chi2_2[i]<5) and Xmax2[i]<650:\n",
    "        good2.append(1)\n",
    "        count+=1\n",
    "    else:\n",
    "        good2.append(0)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "S125_1 = np.log10(file['LaputopParams']['s125'].array())\n",
    "S125_2 = np.log10(file2['LaputopParams']['s125'].array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = file['CurvatureOnlyParams']['A'].array()\n",
    "A2 = file2['CurvatureOnlyParams']['A'].array()\n",
    "D1 = file['CurvatureOnlyParams']['D'].array()\n",
    "D2 = file2['CurvatureOnlyParams']['D'].array()\n",
    "N1 = file['CurvatureOnlyParams']['N'].array()\n",
    "N2 = file2['CurvatureOnlyParams']['N'].array()\n",
    "beta1 = file['LaputopParams']['beta'].array()\n",
    "beta2 = file2['LaputopParams']['beta'].array()\n",
    "zenith1 = file['Laputop']['zenith'].array()\n",
    "zenith2 = file2['Laputop']['zenith'].array()\n",
    "rlogl1 = file['CurvatureOnlyParams']['rlogl'].array()\n",
    "nmini1 = file['CurvatureOnlyParams']['nmini'].array()\n",
    "ndf1 = file['CurvatureOnlyParams']['ndf'].array()\n",
    "llh1 = file['CurvatureOnlyParams']['llh'].array()\n",
    "rlogl2 = file2['CurvatureOnlyParams']['rlogl'].array()\n",
    "nmini2 = file2['CurvatureOnlyParams']['nmini'].array()\n",
    "ndf2 = file2['CurvatureOnlyParams']['ndf'].array()\n",
    "llh2 = file2['CurvatureOnlyParams']['llh'].array()\n",
    "\n",
    "Xmax1_new = Xmax1\n",
    "Xmax2_new = Xmax2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.log10(np.append(A1,A2))\n",
    "D = np.append(D1,D2)\n",
    "N = np.append(N1,N2)\n",
    "S125 = np.append(S125_1,S125_2)\n",
    "beta = np.log10(np.append(beta1,beta2))\n",
    "chi2 = np.append(chi2_1,chi2_2)\n",
    "zenith = np.cos(np.append(zenith1,zenith2))\n",
    "Xmax = np.append(Xmax1_new,Xmax2_new)\n",
    "goodness = np.append(good,good2)\n",
    "red = np.append(red1,red2)\n",
    "mass = np.append(mass1,mass2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.11042152 -0.06389393  0.07506336  0.10801262  0.20482409\n",
      "   0.02356671  0.08135272 -0.01841696 -0.07125706 -0.05894829]\n",
      " [ 0.11042152  1.         -0.0435993  -0.52563202  0.06855834  0.02395401\n",
      "   0.00522173  0.06673223 -0.09536741 -0.14998897  0.0453969 ]\n",
      " [-0.06389393 -0.0435993   1.          0.08137576 -0.0860212  -0.10458563\n",
      "  -0.01237658  0.42102965  0.04476834  0.0769891  -0.03354958]\n",
      " [ 0.07506336 -0.52563202  0.08137576  1.          0.07575187  0.19535921\n",
      "   0.07220079 -0.05334439  0.17610724  0.04298084 -0.14193754]\n",
      " [ 0.10801262  0.06855834 -0.0860212   0.07575187  1.          0.41891977\n",
      "   0.04970585 -0.02705426 -0.04467872 -0.20083333  0.15994521]\n",
      " [ 0.20482409  0.02395401 -0.10458563  0.19535921  0.41891977  1.\n",
      "  -0.04087814 -0.0246141  -0.09956015  0.01339489  0.30870411]\n",
      " [ 0.02356671  0.00522173 -0.01237658  0.07220079  0.04970585 -0.04087814\n",
      "   1.         -0.00294529  0.04444636 -0.19341011 -0.08236207]\n",
      " [ 0.08135272  0.06673223  0.42102965 -0.05334439 -0.02705426 -0.0246141\n",
      "  -0.00294529  1.         -0.00375543  0.00542323 -0.09060697]\n",
      " [-0.01841696 -0.09536741  0.04476834  0.17610724 -0.04467872 -0.09956015\n",
      "   0.04444636 -0.00375543  1.         -0.02061313 -0.28959955]\n",
      " [-0.07125706 -0.14998897  0.0769891   0.04298084 -0.20083333  0.01339489\n",
      "  -0.19341011  0.00542323 -0.02061313  1.          0.04227521]\n",
      " [-0.05894829  0.0453969  -0.03354958 -0.14193754  0.15994521  0.30870411\n",
      "  -0.08236207 -0.09060697 -0.28959955  0.04227521  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(np.corrcoef(np.array([A,D,N,S125,beta,zenith,Xmax,chi2,red,mass,goodness])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_variable = np.array([np.append(np.append(i,j),k) for i,j,k in zip(S125,beta,zenith)])\n",
    "output = list(zip(Xmax,goodness))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "seed = 7\n",
    "test_size = 0.2\n",
    "X_train, X_test, y_train_1, y_test_1 = train_test_split(input_variable,output , test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4552326434757082 0.46006419242972035 5 7\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "y_train = list(zip(*y_train_1))[1]\n",
    "y_test = list(zip(*y_test_1))[1]\n",
    "for i in [5]:\n",
    "    for j in [7]:\n",
    "        rng = np.random.RandomState(1)\n",
    "        regr_1 = AdaBoostRegressor(DecisionTreeRegressor(max_depth=j,criterion='friedman_mse'),\n",
    "                                n_estimators=i, random_state=rng,loss='square')\n",
    "        regr_1.fit(X_train,y_train)\n",
    "    \n",
    "        y_1= regr_1.predict(X_test)\n",
    "        y_2 = regr_1.predict(X_train)\n",
    "\n",
    "        #mse = [(i-j)**2.0 for i,j in zip(y_1,y_test)]\n",
    "        #mse2 = [(i-j)**2.0 for i,j in zip(y_2,y_train)]\n",
    "        count_0 = 0\n",
    "        count_02 = 0\n",
    "        for value in y_train:\n",
    "            if value==0:\n",
    "                count_0+=1\n",
    "        for value in y_test:\n",
    "            if value==0:\n",
    "                count_02+=1\n",
    "        \n",
    "        count = 0\n",
    "        count2 = 0\n",
    "        for value1,value2 in zip(y_1,y_test):\n",
    "            if value1<0.5 and value2==0:\n",
    "                count+=1\n",
    "\n",
    "        for value1,value2 in zip(y_2,y_train):\n",
    "            if value1<0.5 and value2==0:\n",
    "                count2+=1\n",
    "\n",
    "        print(count/count_02,count2/count_0,i,j)\n",
    "        \n",
    "        #residual = [i-j for i,j in zip(y_1,y_test)]\n",
    "        #mean,std=norm.fit(residual)\n",
    "        #print(mean,std,i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = Xmax>300\n",
    "check2 = Xmax<900\n",
    "new_check= []\n",
    "for i,j in zip(check,check2):\n",
    "    if i and j:\n",
    "        new_check.append(True)\n",
    "    else:\n",
    "        new_check.append(False)\n",
    "    \n",
    "mask = regr_1.predict(input_variable)\n",
    "mask_convert_pre = []\n",
    "for i in mask:\n",
    "    if i>=0.5:\n",
    "        mask_convert_pre.append(True)\n",
    "    else:\n",
    "        mask_convert_pre.append(False)\n",
    "\n",
    "mask_convert = []\n",
    "for i,j in zip(new_check,mask_convert_pre):\n",
    "    if i and j:\n",
    "        mask_convert.append(True)\n",
    "    else:\n",
    "        mask_convert.append(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.13216003 -0.06992996  0.07621445  0.05489924  0.15042306\n",
      "   0.09380134  0.15893839 -0.00453584 -0.07362787 -0.17353523]\n",
      " [ 0.13216003  1.         -0.02965211 -0.51452004  0.00223243 -0.13862801\n",
      "  -0.00464256  0.10350347 -0.06472633 -0.15834258 -0.04387069]\n",
      " [-0.06992996 -0.02965211  1.          0.05188534 -0.04363735 -0.04930809\n",
      "  -0.04757658  0.63551711  0.02215767  0.06987297  0.017277  ]\n",
      " [ 0.07621445 -0.51452004  0.05188534  1.          0.11606144  0.37025016\n",
      "   0.28211679 -0.07263374  0.13233394  0.05243804 -0.09357577]\n",
      " [ 0.05489924  0.00223243 -0.04363735  0.11606144  1.          0.26102807\n",
      "   0.22185904 -0.0410097  -0.00821978 -0.19875661  0.05436811]\n",
      " [ 0.15042306 -0.13862801 -0.04930809  0.37025016  0.26102807  1.\n",
      "  -0.05258422 -0.03980583 -0.01332203  0.01897163  0.09013254]\n",
      " [ 0.09380134 -0.00464256 -0.04757658  0.28211679  0.22185904 -0.05258422\n",
      "   1.         -0.01587168  0.05792182 -0.65618207 -0.06744687]\n",
      " [ 0.15893839  0.10350347  0.63551711 -0.07263374 -0.0410097  -0.03980583\n",
      "  -0.01587168  1.         -0.00551813  0.00392349 -0.18052916]\n",
      " [-0.00453584 -0.06472633  0.02215767  0.13233394 -0.00821978 -0.01332203\n",
      "   0.05792182 -0.00551813  1.          0.00791305 -0.24961737]\n",
      " [-0.07362787 -0.15834258  0.06987297  0.05243804 -0.19875661  0.01897163\n",
      "  -0.65618207  0.00392349  0.00791305  1.         -0.00723265]\n",
      " [-0.17353523 -0.04387069  0.017277   -0.09357577  0.05436811  0.09013254\n",
      "  -0.06744687 -0.18052916 -0.24961737 -0.00723265  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(np.corrcoef(np.array([A[mask_convert],\n",
    "                            D[mask_convert],\n",
    "                            N[mask_convert],\n",
    "                            S125[mask_convert],\n",
    "                            beta[mask_convert],\n",
    "                            zenith[mask_convert],\n",
    "                            Xmax[mask_convert],\n",
    "                            chi2[mask_convert],\n",
    "                            red[mask_convert],\n",
    "                            mass[mask_convert],\n",
    "                            goodness[mask_convert]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_new = []\n",
    "import random\n",
    "for i,j in zip(mask_convert,mass):\n",
    "    if (i and j==1) and random.random()<0.15:\n",
    "        mask_new.append(False)\n",
    "    else:\n",
    "        mask_new.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_variable2 = np.array([np.append(i,j) for i,j in zip(D[mask_new],beta[mask_new])])\n",
    "output_new = mass[mask_new]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "seed = 7\n",
    "test_size = 0.2\n",
    "X_train, X_test, y_train_1, y_test_1 = train_test_split(input_variable2,output_new , test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7127461260631481 0.5385925751879699 0.7533991323841965 0.5846176448982591 1 15\n",
      "0.6924734941162763 0.540296052631579 0.7642588872390602 0.6170131491956218 1 18\n",
      "0.7127461260631481 0.5385925751879699 0.7533991323841965 0.5846176448982591 2 15\n",
      "0.6924734941162763 0.540296052631579 0.7642588872390602 0.6170131491956218 2 18\n",
      "0.7128626354421531 0.5503994360902256 0.742160887413748 0.5850584000587673 3 15\n",
      "0.7185715950133986 0.5453477443609023 0.7663260255626401 0.5996327040329097 3 18\n",
      "0.7304555516719096 0.5386513157894737 0.7569365592337031 0.5730845515316242 4 15\n",
      "0.7245135733426541 0.5360667293233082 0.7746236934812356 0.5959744362006906 4 18\n",
      "0.6807060468367704 0.5898143796992481 0.7027979153929018 0.6172629104532432 5 15\n",
      "0.7534661540253991 0.5152725563909775 0.7790927883075669 0.5495923014765298 5 18\n",
      "0.7161248980542934 0.556625939849624 0.7370949427897633 0.586057445089253 6 15\n",
      "0.7383199347547478 0.5316611842105263 0.7792529187410836 0.5784617644898259 6 18\n",
      "0.7019107538156821 0.5704299812030075 0.721183800623053 0.5965180342319841 7 15\n",
      "0.730047768845392 0.5440554511278195 0.7566454129909453 0.5776096378461765 7 18\n",
      "0.7200862169404637 0.5558035714285714 0.7402393222115469 0.5850290163814001 8 15\n",
      "0.7398928113713154 0.5345394736842105 0.7752787725274405 0.5772423418790862 8 18\n",
      "0.682628451590353 0.5875234962406015 0.6989693423006376 0.612811283332109 9 15\n",
      "0.7123383432366306 0.5674929511278195 0.7393367688589979 0.602042165577022 9 18\n",
      "0.6590353023418385 0.6097274436090225 0.6828252831397211 0.6396238889296996 10 15\n",
      "0.7151928230222533 0.5623825187969925 0.7464116225580109 0.601248806288107 10 18\n",
      "0.5570313410229524 0.6880874060150376 0.5798759717005852 0.7158451480202748 11 15\n",
      "0.7337178142840499 0.5454652255639098 0.757722654089149 0.577359876588555 11 18\n",
      "0.5547011534428522 0.6934328007518797 0.5822488135790608 0.7242341879086167 12 15\n",
      "0.7363975300011651 0.5388862781954887 0.769310274550907 0.5783148461029898 12 18\n",
      "0.4420948386345101 0.7489426691729323 0.4751797828049029 0.7842944244472195 13 15\n",
      "0.7430968192939532 0.5368303571428571 0.7680729030191865 0.5672225078968632 13 18\n",
      "0.5586624723290224 0.6677631578947368 0.5932978134917168 0.7082200837434804 14 15\n",
      "0.7301642782243971 0.5464050751879699 0.7647538358517484 0.585939910379784 14 18\n",
      "0.5430502155423511 0.6632401315789473 0.5727428887530206 0.6988907661793874 15 15\n",
      "0.7384364441337528 0.5409421992481203 0.7635019070078901 0.5725409535003305 15 18\n",
      "0.5691483164394734 0.661125469924812 0.6069816869013306 0.7042532872989055 16 15\n",
      "0.7154840964697659 0.5599154135338346 0.7496724604768975 0.600190993902887 16 18\n",
      "0.3741698706745893 0.7774318609022557 0.4109092497161324 0.8136927936531256 17 15\n",
      "0.7263194687172317 0.553453947368421 0.7547966343494337 0.5877910820539191 17 18\n",
      "0.5746825119422113 0.6549577067669173 0.6117273706582816 0.6983324763094101 18 15\n",
      "0.7023767913317022 0.5741893796992481 0.7389000494948613 0.6170131491956218 18 18\n",
      "0.39100547594081325 0.7723801691729323 0.42903310332780153 0.809902299272754 19 15\n",
      "0.7169404637073284 0.5631461466165414 0.7449413340320843 0.5956805994270183 19 18\n",
      "0.5787603402073866 0.6531954887218046 0.6152065682592366 0.6969220597957835 20 15\n",
      "0.741582197366888 0.5316611842105263 0.7793839345503246 0.5774333357819731 20 18\n",
      "0.5444483280904113 0.683329417293233 0.5732669519899846 0.7190920443693528 21 15\n",
      "0.7743213328672958 0.4924812030075188 0.8099834046641629 0.5346507015352971 21 18\n",
      "0.5364674356285681 0.691670582706767 0.5725973156316417 0.731859252185411 22 15\n",
      "0.7807293487125714 0.4891917293233083 0.8209450607039916 0.5379416734004261 22 18\n",
      "0.40545263893743444 0.7707941729323309 0.4426296328645879 0.8083155806949239 23 15\n",
      "0.7927298147500874 0.46898496240601506 0.8289515823798294 0.5130536986703886 23 18\n",
      "0.4978445764884073 0.7289708646616542 0.5290418377150843 0.7619481378094468 24 15\n",
      "0.7865548176628219 0.4810267857142857 0.823725507322328 0.5263204290016895 24 18\n",
      "0.5127577770010486 0.7163416353383458 0.5409642763560136 0.7464041724821862 25 15\n",
      "0.8255272049399976 0.42193374060150374 0.8586776137653943 0.46283699404980533 25 18\n",
      "0.522602819526972 0.7144619360902256 0.5506885608641221 0.7452435172261809 26 15\n",
      "0.787370383315857 0.4796170112781955 0.8239438670043963 0.523734665393374 26 18\n",
      "0.4259582896423162 0.7707354323308271 0.45549829679447984 0.8004701388378755 27 15\n",
      "0.8263427705930327 0.42152255639097747 0.858852301511049 0.4615734959230148 27 18\n",
      "0.5263893743446347 0.7108787593984962 0.5526392406905989 0.7412179534268714 28 15\n",
      "0.8252941861819877 0.41964285714285715 0.8627391038518648 0.4650113861749798 28 18\n",
      "0.40725853431201214 0.7790178571428571 0.439397909569977 0.8117093954308382 29 15\n",
      "0.8555283700337877 0.36407424812030076 0.8902233091681951 0.4067582457944612 29 18\n",
      "0.4113363625771875 0.7784891917293233 0.4470696130666434 0.8130169690736796 30 15\n",
      "0.8273331003145753 0.42169877819548873 0.8597694121757359 0.4617204143098509 30 18\n",
      "0.2777001048584411 0.8455122180451128 0.3126328354732582 0.8821567619187541 31 15\n",
      "0.8634510078061284 0.35502819548872183 0.8955512854106618 0.3967384118122383 31 18\n",
      "0.42432715833624607 0.7738486842105263 0.45775468017585236 0.8091236318225226 32 15\n",
      "0.8671793079342887 0.3479205827067669 0.902466008676158 0.39200763975611547 32 18\n",
      "0.3271583362460678 0.8219572368421053 0.3632631670888287 0.8584882097994564 33 15\n",
      "0.8643830828381684 0.35220864661654133 0.8975601944856901 0.3942995665907588 33 18\n",
      "0.4529884655714785 0.7556390977443609 0.48963519375782455 0.7934327481084258 34 15\n",
      "0.841022952347664 0.39790883458646614 0.8778350365388534 0.4406082421215015 34 18\n",
      "0.3291972503786555 0.8262453007518797 0.36358342795586224 0.860882979504885 35 15\n",
      "0.859489688919958 0.3572015977443609 0.8957987597170058 0.4010431205465364 35 18\n",
      "0.32162414074332984 0.8309445488721805 0.3581972224648441 0.8685080437816792 36 15\n",
      "0.8226144704648725 0.4234022556390977 0.8628992342853815 0.46703885991331817 36 18\n",
      "0.3116625888384015 0.8332354323308271 0.3480944478411506 0.8726364504517741 37 15\n",
      "0.8266340440405453 0.41423872180451127 0.8634232975223455 0.454345111290678 37 18\n",
      "0.447629034137248 0.7619243421052632 0.48133752583922906 0.7995151693234408 38 15\n",
      "0.8101479669113364 0.43344689849624063 0.8558534952106444 0.4812017924043194 38 18\n",
      "0.31941046254223465 0.8266564849624061 0.3563193291990567 0.8686990376845662 39 15\n",
      "0.8094489106373063 0.4387335526315789 0.8501907007890063 0.4817013149195622 39 18\n",
      "0.3127111732494466 0.832295582706767 0.35157364544210556 0.8753691324469257 40 15\n",
      "0.7950600023301876 0.46681156015037595 0.8311060645762366 0.5099096451920958 40 18\n",
      "0.1833275078643831 0.8998472744360902 0.22252307333973856 0.9360023506941894 41 15\n",
      "0.8188861703367121 0.4236372180451128 0.8548053687367164 0.4644237126276353 41 18\n",
      "0.22940696726086449 0.8771734022556391 0.2677671994642909 0.9160508337618453 42 15\n",
      "0.7850984504252593 0.47761983082706766 0.8231141002125367 0.5210166752369059 42 18\n",
      "0.17837585925667016 0.9061912593984962 0.2111683698721868 0.9374421508851832 43 15\n",
      "0.8137597576604917 0.426750469924812 0.8509913529565901 0.46756776610592815 43 18\n",
      "0.24734941162763602 0.8690084586466166 0.2869682941741637 0.9029750973334313 44 15\n",
      "0.8183618781311895 0.4212875939849624 0.857818732349259 0.46534929846470285 44 18\n",
      "0.15227775835954793 0.9181156015037594 0.18642093923777914 0.9486814074781459 45 15\n",
      "0.8271000815565653 0.41118421052631576 0.8636998864529654 0.4539043561301697 45 18\n",
      "0.15554002097168823 0.9160596804511278 0.1915742277345911 0.9484610298978917 46 15\n",
      "0.8268088081090528 0.41059680451127817 0.8644423093719975 0.45619628296481307 46 18\n",
      "0.12099499009670278 0.9318021616541353 0.1548898011471162 0.9617718357452435 47 15\n",
      "0.8215076313643248 0.41517857142857145 0.8572801118001572 0.4566223462866378 47 18\n",
      "0.12151928230222533 0.9327420112781954 0.15628730311235334 0.9623007419378535 48 15\n",
      "0.7894092974484446 0.46534304511278196 0.8287623373220369 0.5087196062587233 48 18\n",
      "0.10986834440172434 0.9393796992481203 0.13960462340233498 0.9661940791890105 49 15\n",
      "0.8251776768029826 0.40625 0.8630448074067604 0.44968779842797324 49 18\n",
      "0.20727018524991261 0.8879229323308271 0.24412612455236266 0.9223242488797473 50 15\n",
      "0.8283816847256205 0.40131578947368424 0.8696538271173611 0.448600602365386 50 18\n",
      "0.2130956542001631 0.8821663533834586 0.24901738143069263 0.916932344082862 51 15\n",
      "0.8490620994990097 0.36430921052631576 0.8879669257868227 0.40790420921178283 51 18\n",
      "0.21012466503553537 0.884515977443609 0.24791102570821325 0.9201498567545728 52 15\n",
      "0.8336246067808458 0.3987312030075188 0.8741374792558302 0.44548593256446045 52 18\n",
      "0.11930560410113014 0.9297462406015038 0.15203656796809037 0.9596268272974363 53 15\n",
      "0.8629849702901083 0.3383458646616541 0.9018546015663668 0.3830162344817454 53 18\n",
      "0.20086216940463708 0.8876292293233082 0.2380993973272775 0.9238228164254757 54 15\n",
      "0.8628102062216009 0.33699483082706766 0.9034559059015344 0.3844707265114229 54 18\n",
      "0.16305487591751136 0.9077185150375939 0.20027950039304743 0.9431719679717917 55 15\n",
      "0.8607130373995107 0.3412828947368421 0.8976475383585175 0.38401527951223097 55 18\n",
      "0.22637772340673423 0.873296522556391 0.2649139662852651 0.9126864027032984 56 15\n",
      "0.8275078643830829 0.40613251879699247 0.866451218447026 0.4499669433629619 56 18\n",
      "0.17860887801468017 0.9029017857142857 0.21154685998777187 0.9357819731139352 57 15\n",
      "0.8211581032273098 0.4131813909774436 0.8586484991411186 0.4564166605450672 57 18\n",
      "0.22317371548409648 0.8757636278195489 0.25849419163245696 0.9129802394769705 58 15\n",
      "0.823837818944425 0.4117128759398496 0.8622878271755903 0.45556453390141777 58 18\n",
      "0.22993125946638704 0.8693609022556391 0.26406964218126766 0.9078821714537575 59 15\n",
      "0.8286729581731329 0.4010808270676692 0.8642385070020672 0.442062734151179 59 18\n",
      "0.24734941162763602 0.8629581766917294 0.28245552741141877 0.9016675236905899 60 15\n",
      "0.8185366421996971 0.41835056390977443 0.8581098785920168 0.4615000367295967 60 18\n",
      "0.1540253990446231 0.9154135338345865 0.17985559146359217 0.945654888709322 61 15\n",
      "0.8121286263544215 0.4263392857142857 0.8508894517716249 0.46794975391170207 61 18\n",
      "0.1530350693230805 0.9171170112781954 0.1806853582554517 0.9474472930287225 62 15\n",
      "0.7936618897821275 0.45999765037593987 0.8318484874952689 0.5023580401087197 62 18\n",
      "0.15559827566119072 0.9153547932330827 0.18084548868896846 0.94519944171013 63 15\n",
      "0.8223814517068624 0.40877584586466165 0.8614143884473171 0.4507309189745097 63 18\n",
      "0.1542584178026331 0.9159421992481203 0.18365504993158063 0.9475648277381914 64 15\n",
      "0.8245368752184551 0.4055451127819549 0.8658834832736484 0.45040769852347023 64 18\n",
      "0.15309332401258302 0.916999530075188 0.17823972981628672 0.9466098582237567 65 15\n",
      "0.8431201211697542 0.3589638157894737 0.8779660523480944 0.3958275178138544 65 18\n",
      "0.21327041826867063 0.8852208646616542 0.24364573325181238 0.9211929773011093 66 15\n",
      "0.8282069206571129 0.3951480263157895 0.8695519259323958 0.4406376257988687 66 18\n",
      "0.1612489805429337 0.9145911654135338 0.18474684834192215 0.9466245500624403 67 15\n",
      "0.8362460678084586 0.38199013157894735 0.8769179258741666 0.4255784911481672 67 18\n",
      "0.22608644995922172 0.8787006578947368 0.2570384604186683 0.9149195621832072 68 15\n",
      "0.8365373412559711 0.3828125 0.8791306373191254 0.4293836773672225 68 18\n",
      "0.1987650005825469 0.8926809210526315 0.22351297056511485 0.9253948431646221 69 15\n",
      "0.8332750786438309 0.3872767857142857 0.8737007598916936 0.4306471754940131 69 18\n",
      "0.20855178841896774 0.8846334586466166 0.23852155937927622 0.9196356424006464 70 15\n",
      "0.8327507864383082 0.39068374060150374 0.8769761551227181 0.4377286417395137 70 18\n",
      "0.170511476173832 0.9052514097744361 0.197761085393193 0.9390435613016969 71 15\n",
      "0.859373179540953 0.33999060150375937 0.8984190759018255 0.38189965474179094 71 18\n",
      "0.23709658627519514 0.870829417293233 0.27264389903048303 0.9083229266142657 72 15\n",
      "0.829605033205173 0.39767387218045114 0.8733077124639707 0.4449570263718504 72 18\n",
      "0.23365955959454737 0.8708881578947368 0.26561271726788366 0.9079703224858591 73 15\n",
      "0.841488989863684 0.3742951127819549 0.8808920720878097 0.4178065084845368 73 18\n",
      "0.27012699522311545 0.8463345864661654 0.30357818732349257 0.884316462205245 74 15\n",
      "0.8227309798438774 0.41459116541353386 0.8636853291408274 0.4600455446999192 74 18\n",
      "0.2744378422463008 0.8447485902255639 0.3058782426412787 0.8822008374348049 75 15\n",
      "0.8444017243388092 0.3762922932330827 0.8846041866829708 0.4206273415117902 75 18\n",
      "0.3523243621111499 0.7971099624060151 0.38923341194281885 0.8379196356424007 76 15\n",
      "0.8252359314924852 0.40977443609022557 0.867979736221504 0.4574157055755528 76 18\n",
      "0.27589420948386345 0.8422227443609023 0.30694092642734444 0.8800852126643649 77 15\n",
      "0.8347314458813935 0.3933858082706767 0.8756514397181704 0.4380371703518695 77 18\n",
      "0.3350809740184085 0.8060385338345865 0.36931900893818964 0.8433849996327041 78 15\n",
      "0.8091576371897938 0.43802866541353386 0.8520685940547937 0.4856240358480864 78 18\n",
      "0.34277059303273916 0.7978735902255639 0.3747052144292078 0.8335561595533681 79 15\n",
      "0.8024583478970058 0.44725093984962405 0.8428392581593734 0.49264673473885257 79 18\n",
      "0.35611091692881275 0.7904135338345865 0.3931930008443241 0.8313670755895101 80 15\n",
      "0.782302225329139 0.48114426691729323 0.8215710251259207 0.5269374862264012 80 18\n",
      "0.3328672958173133 0.8089168233082706 0.3708038547762541 0.8500550943950635 81 15\n",
      "0.799836886869393 0.4491893796992481 0.840713890587242 0.49408653492984644 81 18\n",
      "0.33729465221950367 0.8110314849624061 0.37664133694354673 0.8516565048115772 82 15\n",
      "0.7975066992892927 0.4555333646616541 0.8399714676682097 0.5037831484610299 82 18\n",
      "0.34038215076313644 0.806390977443609 0.3771071709319591 0.8476015573349005 83 15\n",
      "0.788418967726902 0.4654017857142857 0.8286022068885202 0.5110703004481011 83 18\n",
      "0.3570429919608529 0.8015742481203008 0.393076542347221 0.8414309850877837 84 15\n",
      "0.7857392520097868 0.47673872180451127 0.8264477246921128 0.523205759200764 84 18\n",
      "0.3401491320051264 0.8056860902255639 0.3752583922904475 0.8454565488870932 85 15\n",
      "0.7970406617732727 0.4552984022556391 0.8351384400384313 0.5039888342026004 85 18\n",
      "0.38582080857509027 0.782953477443609 0.42172533263458234 0.8197017556747227 86 15\n",
      "0.8043224979610859 0.44542998120300753 0.8461874399510875 0.4967310658928965 86 18\n",
      "0.33601304905044854 0.8115014097744361 0.3708329694005299 0.8477631675604201 87 15\n",
      "0.7975649539787953 0.4542998120300752 0.836987218679943 0.5043120546536399 87 18\n",
      "0.3977047652336013 0.7768444548872181 0.43259964480158386 0.8131051201057813 88 15\n",
      "0.8032739135500407 0.4471921992481203 0.8447608233615745 0.4985822375670315 88 18\n",
      "0.35302341838518003 0.800046992481203 0.38958278743412816 0.8372731947403218 89 15\n",
      "0.7957008039147151 0.4561795112781955 0.8361574518880834 0.505707779328583 89 18\n",
      "0.3757427472911569 0.7926456766917294 0.41297638803971237 0.8287078527877764 90 15\n",
      "0.8019923103809856 0.4488956766917293 0.8456342620898477 0.5009623154337766 90 18\n",
      "0.3730630315740417 0.7919407894736842 0.40952630506303317 0.8272680525967825 91 15\n",
      "0.7963998601887452 0.4527138157894737 0.8370600052406324 0.4994784397267318 91 18\n",
      "0.3967144355120587 0.7758458646616542 0.4328616764200658 0.8096231543377653 92 15\n",
      "0.7769427938949085 0.4850798872180451 0.8191836259353074 0.5337251156982297 92 18\n",
      "0.34399394151229173 0.8079769736842105 0.3792034238798148 0.8422096525380152 93 15\n",
      "0.788185948968892 0.4678688909774436 0.8253268116574956 0.5085579960332035 93 18\n",
      "0.3977047652336013 0.7756109022556391 0.43539464873205813 0.8114302504958496 94 15\n",
      "0.79372014447163 0.4613486842105263 0.8336390368882289 0.505634320135165 94 18\n",
      "0.35442153093324014 0.8018092105263158 0.39160625382129444 0.8382281642547564 95 15\n",
      "0.7863217989048118 0.47045347744360905 0.8245698314263255 0.5127304782193491 95 18\n",
      "0.3988116043341489 0.7814262218045113 0.43357498471482225 0.814779989715713 96 15\n",
      "0.7927880694395899 0.46316964285714285 0.8330567444027135 0.5081466245500624 96 18\n",
      "0.38413142257951766 0.7917645676691729 0.41786764491804235 0.8273415117902005 97 15\n",
      "0.7980892461843179 0.4557683270676692 0.8357061752118089 0.4990083008888562 97 18\n",
      "0.39735523709658627 0.7854793233082706 0.4344193088188197 0.8224932050246089 98 15\n",
      "0.7780496329954562 0.4876644736842105 0.8180918275249658 0.5348123117608169 98 18\n",
      "0.3955493417220086 0.7845982142857143 0.43094011121786474 0.8215235436714905 99 15\n",
      "0.7915064662705348 0.4635808270676692 0.8285876495763822 0.5050760302651877 99 18\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "for i in range(1,100):\n",
    "    for j in [15,18]:\n",
    "        rng = np.random.RandomState(1)\n",
    "        regr_2 = AdaBoostRegressor(DecisionTreeRegressor(max_depth=j,criterion='friedman_mse'),\n",
    "                                        n_estimators=i, random_state=rng,loss='square')\n",
    "        regr_2.fit(X_train,y_train_1)\n",
    "    \n",
    "        new_1= regr_2.predict(X_test)\n",
    "        new_2 = regr_2.predict(X_train)\n",
    "        square = [(i-j)**2.0 for i,j in zip(new_1,y_test_1)]\n",
    "        square2 = [(i-j)**2.0 for i,j in zip(new_2,y_train_1)]\n",
    "        #print(np.mean(square),np.mean(square2),i,j)\n",
    "        mass_predict = []\n",
    "        for k in new_1:\n",
    "            if k <=2.5:\n",
    "                mass_predict.append(1)\n",
    "            elif k>2.5:\n",
    "                mass_predict.append(4)\n",
    "                \n",
    "        mass_predict2 = []\n",
    "        for k in new_2:\n",
    "            if k <=2.5:\n",
    "                mass_predict2.append(1)\n",
    "            elif k>2.5:\n",
    "                mass_predict2.append(4)\n",
    "                \n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        cm=confusion_matrix(y_test_1,mass_predict)\n",
    "        cm2 = confusion_matrix(y_train_1,mass_predict2)\n",
    "        print(cm[0][0]/np.sum(cm[0]),cm[1][1]/np.sum(cm[1]),cm2[0][0]/np.sum(cm2[0]),cm2[1][1]/np.sum(cm2[1]),i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.247797792804042 2.244624531724793 2.4955840831231413 18\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "regr_2 = AdaBoostRegressor(DecisionTreeRegressor(max_depth=15,criterion='friedman_mse'),\n",
    "                            n_estimators=49, random_state=rng,loss='square')\n",
    "regr_2.fit(X_train,y_train_1)\n",
    "    \n",
    "new_1= regr_2.predict(X_test)\n",
    "new_2 = regr_2.predict(X_train)\n",
    "mse = [(m-n)**2.0 for m,n in zip(new_1,y_test_1)]\n",
    "mse2 = [(m-n)**2.0 for m,n in zip(new_2,y_train_1) ]\n",
    "print(np.mean(mse),np.mean(mse2),i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_predict = []\n",
    "for i in new_1:\n",
    "    if i <2.5:\n",
    "        mass_predict.append(1)\n",
    "    else:\n",
    "        mass_predict.append(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1822 15344]\n",
      " [  973 16051]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test_1,mass_predict))\n",
    "cm=confusion_matrix(y_test_1,mass_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10614004427356402 0.9428453947368421\n"
     ]
    }
   ],
   "source": [
    "print(cm[0][0]/np.sum(cm[0]),cm[1][1]/np.sum(cm[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2457277284546606 2.2373313976472424 2.5003564012768464 18\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "regr_3 = AdaBoostRegressor(DecisionTreeRegressor(max_depth=18,criterion='friedman_mse'),\n",
    "                            n_estimators=32, random_state=rng,loss='square')\n",
    "regr_3.fit(X_train,y_train_1)\n",
    "    \n",
    "new_3= regr_3.predict(X_test)\n",
    "new_4 = regr_3.predict(X_train)\n",
    "mse = [(m-n)**2.0 for m,n in zip(new_3,y_test_1)]\n",
    "mse2 = [(m-n)**2.0 for m,n in zip(new_4,y_train_1) ]\n",
    "print(np.mean(mse),np.mean(mse2),i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_predict1 = []\n",
    "for i in new_3:\n",
    "    if i <2.5:\n",
    "        mass_predict1.append(1)\n",
    "    else:\n",
    "        mass_predict1.append(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14841  2325]\n",
      " [11009  6015]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test_1,mass_predict1))\n",
    "cm=confusion_matrix(y_test_1,mass_predict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.864557846906676 0.3533247180451128\n"
     ]
    }
   ],
   "source": [
    "print(cm[0][0]/np.sum(cm[0]),cm[1][1]/np.sum(cm[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
